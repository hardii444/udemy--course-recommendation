# -*- coding: utf-8 -*-
"""UDEMYrs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_e_yBYMSS9km8lEmPiH4tmc3xoFBhBoY
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Read csv file
df = pd.read_csv('/content/udemy_courses.csv')

df.shape
df.columns
df.info()



print(df.info())

# Check for missing values
print(df.isnull().sum())

# Display unique values for each column
print(df.nunique())

# Visualize categorical variables
cat_cols = ['is_paid', 'level', 'subject']

for col in cat_cols:
    plt.figure(figsize=(8, 5))
    sns.countplot(x=col, data=df)
    plt.title(f'Distribution of {col}')
    plt.show()

# Visualize numerical variables
num_cols = ['price', 'num_subscribers', 'num_reviews', 'num_lectures', 'content_duration']

for col in num_cols:
    plt.figure(figsize=(10, 6))
    sns.histplot(df[col], bins=30, kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# Correlation heatmap for numerical variables
plt.figure(figsize=(10, 8))
sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Numerical Variables')
plt.show()

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer

# Convert categorical variables using one-hot encoding
def encode_categorical(df):
    # One-hot encode 'level' and 'subject' columns
    cat_cols = ['level', 'subject']
    encoder = OneHotEncoder()
    encoded_cols = encoder.fit_transform(df[cat_cols])
    encoded_df = pd.DataFrame(encoded_cols.toarray(), columns=encoder.get_feature_names_out(cat_cols))
    df = pd.concat([df, encoded_df], axis=1)
    df.drop(cat_cols, axis=1, inplace=True)
    return df


# Process text data (course_title) using TF-IDF vectorization
def process_text_data(df):
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
    df = pd.concat([df, tfidf_df], axis=1)
    df.drop('course_title', axis=1, inplace=True)
    return df

# Normalize numerical variables
def normalize_numerical(df):
    num_cols = ['price', 'num_subscribers', 'num_reviews', 'num_lectures', 'content_duration']
    scaler = StandardScaler()
    df[num_cols] = scaler.fit_transform(df[num_cols])
    return df

# Feature Engineering Pipeline
def feature_engineering(df):
    # Convert categorical variables
    df = encode_categorical(df)

    # Process text data
    df = process_text_data(df)

    # Normalize numerical variables
    df = normalize_numerical(df)

    return df

# Apply feature engineering to the dataset
df_featured = feature_engineering(df.copy())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Function to compute the cosine similarity matrix
def compute_similarity_matrix(tfidf_matrix):
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
    return cosine_sim

# Function to get recommendations based on similarity scores
def get_recommendations(course_title, cosine_sim, df, top_n=5):
    idx = df[df['course_title'] == course_title].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
    course_indices = [i[0] for i in sim_scores]
    return df.iloc[course_indices]['course_title']

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])

# Compute cosine similarity matrix
cosine_sim = compute_similarity_matrix(tfidf_matrix)

# Example usage
course_title = 'Complete Python Web Course: Build 8 Python Web Apps'
recommendations = get_recommendations(course_title, cosine_sim, df, top_n=5)
print(recommendations)

# Importing necessary libraries
from sklearn.metrics import precision_score, recall_score, f1_score

# Function to compute evaluation metrics
def evaluate_recommendations(actual, predicted):
    precision = precision_score(actual, predicted, average='micro')
    recall = recall_score(actual, predicted, average='micro')
    f1 = f1_score(actual, predicted, average='micro')
    return precision, recall, f1

# Assuming you have the ground truth data for the recommendations


# Evaluate recommendations
precision, recall, f1 = evaluate_recommendations(actual_recommendations, predicted_recommendations)

# Print evaluation metrics
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Read csv file
df = pd.read_csv('/content/udemy_courses.csv')

df.shape
df.columns
df.info()

print(df.info())

# Check for missing values
print(df.isnull().sum())

# Display unique values for each column
print(df.nunique())

# Visualize categorical variables
cat_cols = ['is_paid', 'level', 'subject']

for col in cat_cols:
    plt.figure(figsize=(8, 5))
    sns.countplot(x=col, data=df)
    plt.title(f'Distribution of {col}')
    plt.show()

# Visualize numerical variables
num_cols = ['price', 'num_subscribers', 'num_reviews', 'num_lectures', 'content_duration']

for col in num_cols:
    plt.figure(figsize=(10, 6))
    sns.histplot(df[col], bins=30, kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# Correlation heatmap for numerical variables
plt.figure(figsize=(10, 8))
sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Numerical Variables')
plt.show()

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer

# Convert categorical variables using one-hot encoding
def encode_categorical(df):
    # One-hot encode 'level' and 'subject' columns
    cat_cols = ['level', 'subject']
    encoder = OneHotEncoder()
    encoded_cols = encoder.fit_transform(df[cat_cols])
    encoded_df = pd.DataFrame(encoded_cols.toarray(), columns=encoder.get_feature_names_out(cat_cols))
    df = pd.concat([df, encoded_df], axis=1)
    df.drop(cat_cols, axis=1, inplace=True)
    return df


# Process text data (course_title) using TF-IDF vectorization
def process_text_data(df):
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
    df = pd.concat([df, tfidf_df], axis=1)
    df.drop('course_title', axis=1, inplace=True)
    return df

# Normalize numerical variables
def normalize_numerical(df):
    num_cols = ['price', 'num_subscribers', 'num_reviews', 'num_lectures', 'content_duration']
    scaler = StandardScaler()
    df[num_cols] = scaler.fit_transform(df[num_cols])
    return df

# Feature Engineering Pipeline
def feature_engineering(df):
    # Convert categorical variables
    df = encode_categorical(df)

    # Process text data
    df = process_text_data(df)

    # Normalize numerical variables
    df = normalize_numerical(df)

    return df

# Apply feature engineering to the dataset
df_featured = feature_engineering(df.copy())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Function to compute the cosine similarity matrix
def compute_similarity_matrix(tfidf_matrix):
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
    return cosine_sim

# Function to get recommendations based on similarity scores
def get_recommendations(course_title, cosine_sim, df, top_n=5):
    idx = df[df['course_title'] == course_title].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
    course_indices = [i[0] for i in sim_scores]
    return df.iloc[course_indices]['course_title']

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])

# Compute cosine similarity matrix
cosine_sim = compute_similarity_matrix(tfidf_matrix)

# Example usage
course_title = 'Complete Python Web Course: Build 8 Python Web Apps'
recommendations = get_recommendations(course_title, cosine_sim, df, top_n=5)
print(recommendations)

# Importing necessary libraries
from sklearn.metrics import precision_score, recall_score, f1_score

# Function to compute evaluation metrics
def evaluate_recommendations(actual, predicted):
    precision = precision_score(actual, predicted, average='micro')
    recall = recall_score(actual, predicted, average='micro')
    f1 = f1_score(actual, predicted, average='micro')
    return precision, recall, f1



# Evaluate recommendations
precision, recall, f1 = evaluate_recommendations(actual_recommendations, predicted_recommendations)

# Print evaluation metrics
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Read csv file
df = pd.read_csv('/content/udemy_courses.csv')

df.shape
df.columns
df.info()

print(df.info())

# Check for missing values
print(df.isnull().sum())

# Display unique values for each column
print(df.nunique())

# Visualize categorical variables
cat_cols = ['is_paid', 'level', 'subject']

for col in cat_cols:
    plt.figure(figsize=(8, 5))
    sns.countplot(x=col, data=df)
    plt.title(f'Distribution of {col}')
    plt.show()

# Visualize numerical variables
num_cols = ['price', 'num_subscribers', 'num_reviews', 'num_lectures', 'content_duration']

for col in num_cols:
    plt.figure(figsize=(10, 6))
    sns.histplot(df[col], bins=30, kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# Correlation heatmap for numerical variables
plt.figure(figsize=(10, 8))
sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Numerical Variables')
plt.show()

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer

# Convert categorical variables using one-hot encoding
def encode_categorical(df):
    # One-hot encode 'level' and 'subject' columns
    cat_cols = ['level', 'subject']
    encoder = OneHotEncoder()
    encoded_cols = encoder.fit_transform(df[cat_cols])
    encoded_df = pd.DataFrame(encoded_cols.toarray(), columns=encoder.get_feature_names_out(cat_cols))
    df = pd.concat([df, encoded_df], axis=1)
    df.drop(cat_cols, axis=1, inplace=True)
    return df


# Process text data (course_title) using TF-IDF vectorization
def process_text_data(df):
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
    df = pd.concat([df, tfidf_df], axis=1)
    df.drop('course_title', axis=1, inplace=True)
    return df

# Normalize numerical variables
def normalize_numerical(df):
    num_cols = ['price', 'num_subscribers', 'num_reviews', 'num_lectures', 'content_duration']
    scaler = StandardScaler()
    df[num_cols] = scaler.fit_transform(df[num_cols])
    return df

# Feature Engineering Pipeline
def feature_engineering(df):
    # Convert categorical variables
    df = encode_categorical(df)

    # Process text data
    df = process_text_data(df)

    # Normalize numerical variables
    df = normalize_numerical(df)

    return df

# Apply feature engineering to the dataset
df_featured = feature_engineering(df.copy())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Function to compute the cosine similarity matrix
def compute_similarity_matrix(tfidf_matrix):
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
    return cosine_sim

# Function to get recommendations based on similarity scores
def get_recommendations(course_title, cosine_sim, df, top_n=5):
    idx = df[df['course_title'] == course_title].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
    course_indices = [i[0] for i in sim_scores]
    return df.iloc[course_indices]['course_title']

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])

# Compute cosine similarity matrix
cosine_sim = compute_similarity_matrix(tfidf_matrix)

# Example usage
course_title = 'Complete Python Web Course: Build 8 Python Web Apps'
recommendations = get_recommendations(course_title, cosine_sim, df, top_n=5)
print(recommendations)



"""
NEW PREDICTION"""

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')

# Compute TF-IDF matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])

# Compute cosine similarity matrix
cosine_sim = compute_similarity_matrix(tfidf_matrix)

# Get recommendations for the course title 'Complete Python Web Course: Build 8 Python Web Apps'
course_title = 'Forex Trading MAKE YOUR FIRST TRADE TODAY!'
recommendations = get_recommendations(course_title, cosine_sim, df, top_n=5)

# Print the recommendations
print("Recommendations for '{}' course:".format(course_title))
for i, title in enumerate(recommendations):
    print("{}. {}".format(i+1, title))

course_title = 'Complete Python Web Course: Build 8 Python Web Apps'

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')

# Compute TF-IDF matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])

# Compute cosine similarity matrix
cosine_sim = compute_similarity_matrix(tfidf_matrix)

# Get recommendations for the course title 'Complete Python Web Course: Build 8 Python Web Apps'
course_title = '1 Hour CSS'
recommendations = get_recommendations(course_title, cosine_sim, df, top_n=5)

# Print the recommendations
print("Recommendations for '{}' course:".format(course_title))
for i, title in enumerate(recommendations):
    print("{}. {}".format(i+1, title))

course_title = 'Complete Python Web Course: Build 8 Python Web Apps'

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')

# Compute TF-IDF matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])

# Compute cosine similarity matrix
cosine_sim = compute_similarity_matrix(tfidf_matrix)

# Get recommendations for the course title 'Complete Python Web Course: Build 8 Python Web Apps'
course_title = 'JavaScript: Understanding the Weird Parts'
recommendations = get_recommendations(course_title, cosine_sim, df, top_n=5)

# Print the recommendations
print("Recommendations for '{}' course:".format(course_title))
for i, title in enumerate(recommendations):
    print("{}. {}".format(i+1, title))

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')

# Compute TF-IDF matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(df['course_title'])

# Compute cosine similarity matrix
cosine_sim = compute_similarity_matrix(tfidf_matrix)

# Get recommendations for the course title 'Complete Python Web Course: Build 8 Python Web Apps'
course_title = 'The Web Developer Bootcamp'
recommendations = get_recommendations(course_title, cosine_sim, df, top_n=5)

# Print the recommendations
print("Recommendations for '{}' course:".format(course_title))
for i, title in enumerate(recommendations):
    print("{}. {}".format(i+1, title))

import pandas as pd

# Read the dataset

# Define ground truth recommendations based on number of subscribers and number of reviews
ground_truth_subscribers = df.sort_values(by='num_subscribers', ascending=False)['course_title'].head(5).tolist()
ground_truth_reviews = df.sort_values(by='num_reviews', ascending=False)['course_title'].head(5).tolist()

print("Ground Truth Recommendations based on Number of Subscribers:")
print(ground_truth_subscribers)

print("\nGround Truth Recommendations based on Number of Reviews:")
print(ground_truth_reviews)

import pandas as pd

# Read the dataset

# Define function to get predicted recommendations based on number of reviews
def get_predicted_recommendations(df, column='num_reviews', top_n=5):
    predicted_recommendations = df.sort_values(by=column, ascending=False)['course_title'].head(top_n).tolist()
    return predicted_recommendations

# Get predicted recommendations based on number of reviews
predicted_recommendations_reviews = get_predicted_recommendations(df, column='num_reviews')

# Print predicted recommendations based on number of reviews
print("Predicted Recommendations based on Number of Reviews:")
print(predicted_recommendations_reviews)

from sklearn.metrics import precision_score, recall_score, f1_score

# Function to compute evaluation metrics
def evaluate_recommendations(actual, predicted):
    precision = precision_score(actual, predicted, average='micro')
    recall = recall_score(actual, predicted, average='micro')
    f1 = f1_score(actual, predicted, average='micro')
    return precision, recall, f1

# Assuming you have the ground truth data for the recommendations
# Define ground truth recommendations based on number of subscribers and number of reviews
ground_truth_reviews = df.sort_values(by='num_reviews', ascending=False)['course_title'].head(5).tolist()


# Evaluate recommendations based on number of reviews
precision_reviews, recall_reviews, f1_reviews = evaluate_recommendations(ground_truth_reviews, predicted_recommendations_reviews)



# Print evaluation metrics for recommendations based on number of reviews
print("\nEvaluation Metrics for Recommendations based on Number of Reviews:")
print("Precision:", precision_reviews)
print("Recall:", recall_reviews)
print("F1-score:", f1_reviews)